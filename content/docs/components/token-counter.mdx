---
title: Token Counter
description: A small, elegant UI component that counts tokens in text for AI applications.
---

import {
  BasicDemo,
  WithLimitDemo,
  PositionedDemo,
  TokenizerDemo,
  CustomTokenizerDemo,
  PropsTable,
} from "@/components/demos/token-counter-demos";

## Installation

```bash
npx shadcn@latest add "https://haberui.com/h/token-counter.json"
```

## Overview

Token Counter provides a clean, simple interface for displaying token counts in AI applications. It includes:

- Automatic token counting with approximations for popular models
- Visual progress bar showing token usage
- Warning indicators when approaching limits
- Flexible positioning options
- Support for custom tokenization methods

## Examples

### Basic Usage

<div className="not-prose my-6">
  <BasicDemo />
</div>

### With Token Limit

<div className="not-prose my-6">
  <WithLimitDemo />
</div>

### Positioned Counter

<div className="not-prose my-6">
  <PositionedDemo />
</div>

### Different Tokenizers

<div className="not-prose my-6">
  <TokenizerDemo />
</div>

### Custom Tokenizer

<div className="not-prose my-6">
  <CustomTokenizerDemo />
</div>

## Props

<PropsTable />

## Tokenization Notes

The token counters in this component use approximations based on average characters per token:

- GPT-3.5/GPT-3: ~4 characters per token
- GPT-4: ~3.5 characters per token
- Claude: ~4 characters per token
- LLaMA: ~4.5 characters per token

For precise token counting, consider using a custom tokenizer function with the model's actual tokenization algorithm.
