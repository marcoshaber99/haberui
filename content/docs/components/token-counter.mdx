---
title: Token Counter
description: A small, elegant UI component that counts tokens in text for AI applications.
---

import {
  BasicDemo,
  WithLimitDemo,
  PositionedDemo,
  TokenizerDemo,
  CustomTokenizerDemo,
} from "@/components/demos/token-counter-demos";
import { TypeTable } from "fumadocs-ui/components/type-table";

## Installation

```bash
npx shadcn@latest add "https://haberui.com/h/token-counter.json"
```

## Overview

Token Counter provides a clean, simple interface for displaying token counts in AI applications. It includes:

- Automatic token counting with approximations for popular models
- Visual progress bar showing token usage
- Warning indicators when approaching limits
- Flexible positioning options
- Support for custom tokenization methods

## Examples

### Basic Usage

<div className="not-prose my-6">
  <BasicDemo />
</div>

### With Token Limit

<div className="not-prose my-6">
  <WithLimitDemo />
</div>

### Positioned Counter

<div className="not-prose my-6">
  <PositionedDemo />
</div>

### Different Tokenizers

<div className="not-prose my-6">
  <TokenizerDemo />
</div>

### Custom Tokenizer

<div className="not-prose my-6">
  <CustomTokenizerDemo />
</div>

## Props

<TypeTable
  type={{
    text: {
      description: "The text to count tokens from",
      type: "string",
      required: true,
    },
    maxTokens: {
      description: "Maximum token limit",
      type: "number",
      default: "4096",
    },
    tokenizer: {
      description: "The tokenizer to use (defaults to GPT-3.5 approximation)",
      type: '"gpt3" | "gpt4" | "claude" | "llama" | "custom"',
      default: '"gpt3"',
    },
    customTokenCounter: {
      description:
        "Custom function to count tokens (used when tokenizer is 'custom')",
      type: "(text: string) => number",
    },
    showWarning: {
      description: "Show warning when approaching token limit",
      type: "boolean",
      default: "true",
    },
    warningThreshold: {
      description: "Percentage of max tokens that triggers the warning state",
      type: "number",
      default: "0.85",
    },
    position: {
      description: "Position of the counter",
      type: '"top-right" | "top-left" | "bottom-right" | "bottom-left" | "inline"',
      default: '"inline"',
    },
    showProgress: {
      description: "Show progress bar",
      type: "boolean",
      default: "true",
    },
    label: {
      description: "Label for token count",
      type: "string",
      default: '"tokens"',
    },
    progressClassName: {
      description: "Custom class for progress bar",
      type: "string",
    },
  }}
/>

## Tokenization Notes

The token counters in this component use approximations based on average characters per token:

- GPT-3.5/GPT-3: ~4 characters per token
- GPT-4: ~3.5 characters per token
- Claude: ~4 characters per token
- LLaMA: ~4.5 characters per token

For precise token counting, consider using a custom tokenizer function with the model's actual tokenization algorithm.
